---
# PersistentVolumeClaim for Ollama model storage
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-data
  namespace: mcp-governance
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: llm
    app.kubernetes.io/part-of: mcp-governance
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 6Gi
---
# Ollama Deployment - Local LLM for AI-powered governance scoring
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: mcp-governance
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: llm
    app.kubernetes.io/part-of: mcp-governance
spec:
  replicas: 1
  strategy:
    type: Recreate  # Recreate to avoid model storage conflicts
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama
        app.kubernetes.io/component: llm
        app.kubernetes.io/part-of: mcp-governance
    spec:
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
              name: http
              protocol: TCP
          env:
            - name: OLLAMA_HOST
              value: "0.0.0.0"
          resources:
            requests:
              cpu: "250m"
              memory: "1Gi"
            limits:
              cpu: "2"
              memory: "4Gi"
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
          readinessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 15
            periodSeconds: 30
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data
---
# Ollama Service
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: mcp-governance
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: llm
    app.kubernetes.io/part-of: mcp-governance
spec:
  selector:
    app.kubernetes.io/name: ollama
  ports:
    - port: 11434
      targetPort: 11434
      protocol: TCP
      name: http
  type: ClusterIP
---
# Job to pull the qwen2.5:7b model after Ollama starts
apiVersion: batch/v1
kind: Job
metadata:
  name: ollama-pull-qwen2.5-7b
  namespace: mcp-governance
  labels:
    app.kubernetes.io/name: ollama-model-pull
    app.kubernetes.io/component: llm
    app.kubernetes.io/part-of: mcp-governance
spec:
  backoffLimit: 5
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama-model-pull
    spec:
      restartPolicy: OnFailure
      initContainers:
        # Wait for Ollama to be ready before pulling the model
        - name: wait-for-ollama
          image: busybox:1.36
          command:
            - sh
            - -c
            - |
              echo "Waiting for Ollama to be ready..."
              until wget -qO- http://ollama.mcp-governance.svc.cluster.local:11434/api/tags >/dev/null 2>&1; do
                echo "Ollama not ready yet, retrying in 5s..."
                sleep 5
              done
              echo "Ollama is ready!"
      containers:
        - name: pull-model
          image: curlimages/curl:8.5.0
          command:
            - sh
            - -c
            - |
              echo "Pulling qwen2.5:7b model..."
              curl -s -X POST http://ollama.mcp-governance.svc.cluster.local:11434/api/pull \
                -H "Content-Type: application/json" \
                -d '{"name": "qwen2.5:7b", "stream": false}'
              echo ""
              echo "Model pull complete. Verifying..."
              curl -s http://ollama.mcp-governance.svc.cluster.local:11434/api/tags | grep -o '"qwen2.5:7b"' && \
                echo "✅ qwen2.5:7b model is ready!" || \
                echo "⚠️ Model may still be loading, check Ollama logs"
